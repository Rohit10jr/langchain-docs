{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f80d7e8c",
   "metadata": {},
   "source": [
    "**Multimodality**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "052d4af0",
   "metadata": {},
   "source": [
    "*OverView*\n",
    "\n",
    "Multimodality refers to the ability to work with data that comes in different forms, such as text, audio, images, and video. Multimodality can appear in various components, allowing models and systems to handle and process a mix of these data types seamlessly.\n",
    "\n",
    "* Chat Models: These could, in theory, accept and generate multimodal inputs and outputs, handling a variety of data types like text, images, audio, and video.\n",
    "* Embedding Models: Embedding Models can represent multimodal content, embedding various forms of data—such as text, images, and audio—into vector spaces.\n",
    "* Vector Stores: Vector stores could search over embeddings that represent multimodal data, enabling retrieval across different types of information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5faa7fb",
   "metadata": {},
   "source": [
    "*What kind of multimodality is supported?*\n",
    "\n",
    "Inputs\n",
    "Some models can accept multimodal inputs, such as images, audio, video, or files. The types of multimodal inputs supported depend on the model provider. For instance, OpenAI, Anthropic, and Google Gemini support documents like PDFs as inputs.\n",
    "\n",
    "The gist of passing multimodal inputs to a chat model is to use content blocks that specify a type and corresponding data. For example, to pass an image to a chat model as URL:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c559ef3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "message = HumanMessage(\n",
    "    content=[\n",
    "        {\"type\": \"text\", \"text\": \"Describe the weather in this image:\"},\n",
    "        {\n",
    "            \"type\": \"image\",\n",
    "            \"source_type\": \"url\",\n",
    "            \"url\": \"https://...\",\n",
    "        },\n",
    "    ],\n",
    ")\n",
    "response = model.invoke([message])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b48a1c6f",
   "metadata": {},
   "source": [
    "We can also pass the image as in-line data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cc1c7f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "message = HumanMessage(\n",
    "    content=[\n",
    "        {\"type\": \"text\", \"text\": \"Describe the weather in this image:\"},\n",
    "        {\n",
    "            \"type\": \"image\",\n",
    "            \"source_type\": \"base64\",\n",
    "            \"data\": \"<base64 string>\",\n",
    "            \"mime_type\": \"image/jpeg\",\n",
    "        },\n",
    "    ],\n",
    ")\n",
    "response = model.invoke([message])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "540897a5",
   "metadata": {},
   "source": [
    "To pass a PDF file as in-line data (or URL, as supported by providers such as Anthropic), just change \"type\" to \"file\" and \"mime_type\" to \"application/pdf\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2418fb9c",
   "metadata": {},
   "source": [
    "Most chat models that support multimodal image inputs also accept those values in OpenAI's Chat Completions format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3fc38bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "message = HumanMessage(\n",
    "    content=[\n",
    "        {\"type\": \"text\", \"text\": \"Describe the weather in this image:\"},\n",
    "        {\"type\": \"image_url\", \"image_url\": {\"url\": image_url}},\n",
    "    ],\n",
    ")\n",
    "response = model.invoke([message])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
